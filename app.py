import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from pathlib import Path
import zipfile
import shutil
import concurrent.futures

# NinaPro ã®ãƒ™ãƒ¼ã‚¹ URL
BASE_URL_PREFIX = "https://ninapro.hevs.ch/instructions/"

# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å…ˆã®ãƒ™ãƒ¼ã‚¹ãƒ•ã‚©ãƒ«ãƒ€
BASE_SAVE_DIR = Path.home() / "Desktop" / "NinaPro_Zips"

# DB1ã€œDB10ã‚’é †ç•ªã«å‡¦ç†
for db_index in range(1, 11):
    db_name = f"DB{db_index}"
    page_url = f"{BASE_URL_PREFIX}{db_name}.html"
    print(f"\nğŸ“„ {db_name} ã‚’å‡¦ç†ä¸­: {page_url}")

    db_dir = BASE_SAVE_DIR / db_name
    os.makedirs(db_dir, exist_ok=True)

    try:
        response = requests.get(page_url)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        zip_links = []
        for a_tag in soup.find_all("a", href=True):
            href = a_tag['href']
            if href.endswith(".zip"):
                full_url = urljoin(page_url, href)
                zip_links.append(full_url)

        print(f"  ğŸ”— {len(zip_links)} å€‹ã®ZIPãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")

        # ä¸¦åˆ—ã§ã®ZIPãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–¢æ•°
        def download_and_extract_zip(link):
            filename = os.path.basename(link)
            zip_path = db_dir / filename
            extract_dir = db_dir / filename.replace(".zip", "")

            try:
                print(f"    â¬‡ï¸ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­: {filename}")
                with requests.get(link, stream=True) as r:
                    r.raise_for_status()
                    with open(zip_path, 'wb') as f:
                        for chunk in r.iter_content(chunk_size=8192):
                            f.write(chunk)
                print(f"    âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {zip_path}")

                print(f"    ğŸ“¦ è§£å‡ä¸­...")
                with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                    zip_ref.extractall(extract_dir)
                print(f"    ğŸ“ è§£å‡å®Œäº†: {extract_dir}")

            except Exception as e:
                print(f"    âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰/è§£å‡å¤±æ•—: {link} ã‚¨ãƒ©ãƒ¼: {e}")

        # ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«ã§ä¸¦åˆ—å®Ÿè¡Œ
        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
            executor.map(download_and_extract_zip, zip_links)

        # ä»¥ä¸‹ã¯.matçµ±åˆå‡¦ç†ã¯ãã®ã¾ã¾
        all_mat_dir = db_dir / "all_mat"
        os.makedirs(all_mat_dir, exist_ok=True)
        mat_count = 0

        for folder in db_dir.iterdir():
            if folder.is_dir() and folder.name.lower().startswith("s"):
                for file in folder.glob("*.mat"):
                    dest_file = all_mat_dir / file.name
                    try:
                        shutil.copy(file, dest_file)
                        mat_count += 1
                    except Exception as e:
                        print(f"    âš ï¸ ã‚³ãƒ”ãƒ¼å¤±æ•—: {file} â†’ {dest_file} ã‚¨ãƒ©ãƒ¼: {e}")

        print(f"  âœ… {mat_count} å€‹ã® .mat ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ {all_mat_dir} ã«çµ±åˆã—ã¾ã—ãŸã€‚")

    except Exception as e:
        print(f"  âš ï¸ ãƒšãƒ¼ã‚¸å–å¾—å¤±æ•—: {page_url} ã‚¨ãƒ©ãƒ¼: {e}")
